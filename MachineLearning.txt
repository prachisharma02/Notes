Machine learning

1.1  What is Machine Learning
	Machine learning (ML) is a subdomain of artificial intelligence (AI) that focuses on developing systems that learn—or improve performance—based on the data they ingest. 
	Machine learning is a field of technology  that allows machine the ability  to learn and improve their performance on a specific task without being explicitly programmed
	Machine learning is like teaching a computer to perform a task by showing it a large number of examples. Instead of explicitly telling the computer how to do the task, we let machines to figure out the patterns and rules on its own by the given data.

1.2 How does Machine Learning work
A machine learning system builds prediction models, learns from previous data, and predicts the output of new data whenever it receives it. The amount of data helps to build a better model that accurately predicts the output, which in turn affects the accuracy of the predicted output.

1.3 Feature of Machine learning
	Machine can learn itself from past data and automatically improve.
	From the given dataset it detects various patterns on data.
	It will become more easy to target relatable customer base.
1.4 Why Machine learning 
	Data Abundance: In the digital age, vast amounts of data are generated every day. Machine learning algorithms can extract patterns from this data, which would be impossible for humans to process manually.
        By which many complex problems can be resolved
	Automation: Machine learning enables automation of tasks and processes. It can replace manual and repetitive tasks,
	Hidden Patterns: Businesses and services increasingly aim to provide personalized experiences to users by Finding hidden patterns and extracting useful information from the user data.
	Decision making in various sector including finance

1.5 Types of machine learning problems
Supervised Learning: 
	sample labelled data are provided to the machine learning system for training, and the system then predicts the output based on the training data.
	The system uses labelled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output.
	Types of problems: Classification (categorizing data into classes) and Regression (predicting a continuous value).
	Example: Example: Handwriting Recognition where 1,2,3,4,5 are labels and images are their corresponding output 

Unsupervised Learning: 
	Deals with unlabeled data. 
	The algorithm learns patterns and structures in the data without explicit guidance.
	input data does not have corresponding labels or categories
	algorithm needs to act on that data without any supervision
	Types of problems: Clustering (grouping similar data points) and Dimensionality Reduction (reducing the number of features while retaining important information). 
	Example: Customer Segmentation it group customers into segments like "high-value shoppers," "young and budget-conscious customers," and "occasional buyers. By finding patterns And this is used to target to specific customer groups based on their characteristics and behaviors. 
Reinforcement Learning: 
	feedback-based learning method,
	The agent learns automatically with these feedbacks and improves its performance.
	a learning agent gets a reward for each right action and gets a penalty for each wrong action. 
	The goal of an agent is to get the most reward points, and hence, it improves its performance.
	Types of problems: Games (e.g., chess, Go), robotics, and autonomous systems.
	The robotic dog, which automatically learns the movement of his arms,
1.6  Lifecycle of ml
	Gathering of data
        The goal of this step is to identify and obtain all data-related problems.
        We need to identify the different data sources, as data can be collected from various sources such as files, database, internet, or mobile devices
	Collect data
        The quantity and quality of the collected data will determine the efficiency of the output.
        The more will be the data, the more accurate will be the prediction.
        Integrate the data obtained from different sources
	Data preparation
        Put  data into a suitable place and prepare it to use in our machine learning training.
        In this step, first, we put all data together, and then randomize the ordering of data.
        This step can be further divided into two processes:
	Data exploration:
        It is used to understand the nature of data the characteristics, format, and quality of data.
        A better understanding of data leads to an effective outcome. 
        find Correlations, general trends, and outliers.
	Data pre-processing:
        changing raw data into a configuration reasonable for model training
        data cleaning to eliminate irregularities or blunders, standardization to scale data inside a particular reach, highlight scaling
	Data Wrangling
        Data wrangling is the process of cleaning and converting raw data into a useable format. 
        It is the process of cleaning the data, selecting the variable to use, and transforming the data in a proper format to make it more suitable for analysis in the next step. 
        It is one of the most important steps of the complete process.
        Cleaning of data is required to address the quality issues.
        Sometimes few of the data may not be useful. So they can have various issues, including:
        Missing Values
        Duplicate data
        Invalid data
        Noise
        various filtering techniques is used to  clean the data.
        It is mandatory to detect and remove the above issues because it can negatively affect the quality of the outcome.
	Data Analysis
        This step involves:
        Selection of analytical techniques
        Building models
        Review the result
        The aim of this step is to build a machine learning model to analyse  the data using various analytical techniques and review the outcome. 
        It starts with the determination of the type of the problems, where we select the machine learning techniques such as Classification, Regression, Cluster analysis, Association, etc. then build the model using prepared data, and evaluate the model.
        So here we take the data and use machine learning algorithms to build the model.
	 Train Model
        We train our model to improve its performance for better outcome of the problem.
        We provide datasets to train the model using various machine learning algorithms. 
        Training a model is required so that it can understand the various patterns, rules, and, features.

	 Test Model
        In this step, we check for the accuracy of our model by providing a test dataset to it.
        Testing the model determines the percentage accuracy of the model as per the requirement of project or problem.
7. Deployment
	Here we deploy the model in the real-world system.
	If the above-prepared model is producing an accurate result as per our requirement with acceptable speed, then we deploy the model in the real system. 
	Before deploying check whether it is improving its performance using available data or not. 
	The deployment phase is similar to making the final report for a project.

1.7 Data pre-processing (handling missing values)
https://www.javatpoint.com/data-preprocessing-machine-learning

1.8 labels and features
	Let's first build the model by training it. During training, we provide the model with features and after prediction what we get is label . 
	For example, if we have a dataset containing information like age, weight, height, gender, and calories burned, these are the features. 
	What we want to predict, in this case, is how many calories were burned. So, the calories burned would be our output, and we call it the label. 
	If we were to predict the gender, then gender would be our label
	Features: all the columns like age salary input 
	Labels: output data 



1.9 Steps Involved in Supervised Learning:
	First Determine the type of training dataset 
	Collect/Gather the labelled training data. 
	Split the training dataset into training dataset, test dataset, and validation dataset.
Example : house price :
rows : different houses
columns: different features : area , bedroom ,sale price
x: columns such as  area ,bedroom or independent variable
y= columns such as sale price that we will predict a dependent variable 
x_train: 80% of the x to teach the model 
y_train: 80% of the y to teach for the corresponding value of x
x_test : 20%  of the x to test 
y_test: predict the sale price(y) corresponding to its x_test 
while building/teaching model  we provide model with x_train. y_train , x_test then at the end we test the model by making it predict the x_test and check if its producing correct y_test 
	Determine the input features of the training dataset, which should have enough knowledge so that the model can accurately predict the output.
	Determine the suitable algorithm for the model, such as support vector machine, decision tree, etc.
	Execute the algorithm on the training dataset. Sometimes we need validation sets as the control parameters, which are the subset of training datasets.
	Evaluate the accuracy of the model by providing the test set. If the model predicts the correct output, which means our model is accurate.



	Regression Analysis
	Regression analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. 
	It helps us to understand how the value of the dependent variable is changing corresponding to an independent variable when other independent variables are held fixed. 
	It predicts continuous/real values such as temperature, age, salary, price, etc.
	Regression is a supervised learning technique which helps in finding the correlation between variables and enables us to predict the continuous output variable based on the one or more predictor variables.
	Regression shows a line or curve that passes through all the datapoints on target-predictor graph in such a way that the vertical distance between the datapoints and the regression line is minimum



	Terminologies Related to the Regression Analysis:
	Dependent Variable: The main factor in Regression analysis which we want to predict or understand is called the dependent variable. It is also called target variable.
	Independent Variable: The factors which affect the dependent variables or which are used to predict the values of the dependent variables are called independent variable, also called as a predictor.
	Outliers: Outlier is an observation which contains either very low value or very high value in comparison to other observed values. An outlier may hamper the result, so it should be avoided.
	Multicollinearity: If the independent variables are highly correlated with each other than other variables, then such condition is called Multicollinearity. It should not be present in the dataset, because it creates problem while ranking the most affecting variable.
	Underfitting and Overfitting: If our algorithm works well with the training dataset but not well with test dataset, then such problem is called Overfitting. And if our algorithm does not perform well even with training dataset, then such problem is called underfitting.

	Linear Regression
	Statistical method that is used for predictive analysis. 
	It makes predictions for continuous/real or numeric variables such as sales, salary, age, product price
	Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables. 
	Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.
	The linear regression model provides a sloped straight line representing the relationship between the variables. 

	Positive Linear Relationship: dependent variable increases on the Y-axis and independent variable increases on X-axis
	Negative Linear Relationship: dependent variable decreases on the Y-axis and independent variable increases on the X-axis
	Mathematically 	
y=mx+b
	Where:
	y is the dependent variable (the variable you are trying to predict).
	x is the independent variable (the feature used for prediction).
	m is the slope of the line, which represents the rate of change of y with respect to x.
	b is the y-intercept, which is the value of y when x is zero.
	A linear line showing the relationship between the dependent and independent variables is called a regression line or best fit line

The best fit line:
	It represents the linear equation that best fits a given set of data points.
	It ensures the error between predicted values and actual values should be minimized. 
	The best fit line will have the least error.
	The different values for weights gives a different line of regression
	So best values are calculated for best fit by using cost function
Cost Function
	Mathematical function that measures the difference between predicted values and the actual values in dataset.
	The purpose of a cost function is to quantify how well or poorly a machine learning model is performing in terms of its ability to make predictions. 
	The model aims to minimize this cost function during the training process by adjusting its internal parameters (weights and biases) to make better predictions.

Calculate the Cost Function (Mean Squared Error)
	The cost function (mean squared error) measures the difference between the predicted values and the actual values. 
	It is used to assess how well the best-fit line fits the data.
	The formula for mean squared error (MSE) is: 
Least square
	The Least Squares Method is used to derive a generalized linear equation between two variables, one of which is independent and the other dependent on the former
	A statistical method that is used to find the equation of the line of best fit related to the given data. This method aims at reducing the sum of squares of deviations as much as possible
	Limitation: The least squares method assumes that the data is evenly distributed and doesn’t contain any outliers for deriving a line of best fit
	Steps:
	Step 1: Denote the independent variable values as xi and the dependent ones as yi.
	Step 2: Calculate the average values of xi and yi as X and Y.
	Step 3: Presume the equation of the line of best fit as y = mx + c, where m is the slope of the line and c represents the intercept of the line on the Y-axis.
	Step 4: The slope m can be calculated from the following formula:
	m = [Σ (X – xi)×(Y – yi)]/ Σ(X – xi)2
	c = Y – mX
	Example :https://www.geeksforgeeks.org/least-square-method/

Residual Sum of Squares (RSS)
	Residual sum of squares is used to calculate the variance of the data in terms of error or residuals.
	It is used to calculate the error left between regression data and regression function after running the model. 
	The smaller the value of the residual sum of squares, the better the model.
	 	
	yi is the ith value of variable to be predicted,
	f(xi) is the predicted value, and
	n is the number of terms or variables9.

Regression Sum of Squares (SSR)
	The regression sum of squares measures how well the model is and how close is the predicted value to the expected value.
	 
	Xi is the ith observation of the set,
	 is the mean of the dataset, and
	n is the number of observations.

Total Sum of Squares (TSS)
	It is used to denote the amount of variation in the dependent variable. 
	It is the sum of the regression sum of squares and the residual sum of squares.
	It is calculated as:
	TSS = RSS + SSR

	In linear Regression the checks which ensures to get the best possible result from the given dataset. 
	Linear relationship between the features and target:
	Linear relationship between the dependent and independent variables.
	Small or no multicollinearity between the features:
	Multicollinearity means high-correlation between the independent variables. 
	Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables
	it is difficult to determine which predictor variable is affecting the target variable and which is not. 
Types of Linear Regression
	Simple Linear Regression:
	    If a single independent variable is used to predict the value of a numerical dependent variable
	Multiple Linear regression:
	    If more than one independent variable is used to predict the value of a numerical dependent variable

Simple Linear Regression
	Relationship between a dependent variable and a single independent variable. 
	The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression.
	Dependent variable must be a continuous/real value.
	The independent variable can be measured on continuous or categorical values.
	Objective
	Model the relationship between the two variables. Such as the relationship between Income and expenditure, experience and Salary, etc.
	Forecasting new observations. Such as Weather forecasting according to temperature, 
	can be represented using the below equation:
	y= a1x+c

Multiple Linear Regression
	Algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.
	Uses Two or more independent variables to predict the same dependent variable 
	The goal is to model a linear relationship between the dependent variable and multiple independent variables.
	The equation for multiple linear regression can be written as:
	y=b0+b1x1+b2x2+b3x3+…+bnxn

3.1 Classification 
	Supervised Learning technique that is used to identify the category of new observations on the basis of training data.
	Program learns from the given dataset and then classifies new observation into a number of classes or groups. Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc.
	Classes can be called as targets/labels or categories
	The output variable  is a category, not a value, such as "Green or Blue", "fruit or animal", etc. 
	It takes labelled input data, which means it contains input with the corresponding output.
	Discrete output function(y) is mapped to input variable(x).
	y=f(x), where y = categorical output  
	The main goal is to identify the category of a given dataset
	
                                      
	
