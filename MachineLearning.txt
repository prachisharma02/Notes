Machine learning


Important Terms
NP : Non polynomial whose time complexity grows exponentially is used as np problem mostly it is seen in uninformed search where we do not have knowledge of the domain and we define all the states where np problem occurs like ex 8 puzzle problem   Worst case complexity of 8 puzzle problem is b^d which is  3^20 and in 15 puzzle its 10^13 this is what np is
Heuristic :Technique designed to solve a problem This mostly reduces the time complexity by converting np to p and thus reduces the time complexity. For example, we look at the depth first search,me We see whose cost is less and we go deeper into that side. The other side we don’t go this is what the heuristic used in informed search which gives us optimal values. cost is called as heuristic value
UNIT-1
1.1	State Space Search 
•	No of states in which a problem can go
•	 efn
•	It is a problem-solving technique used in Artificial Intelligence (AI) to find the solution path from the initial state to the goal state by exploring the various states. 
•	The state space search approach searches through all possible states of a problem to find a solution.
•	A way to mathematically represent a problem by defining all the possible states in which the problem can be. 
•	This is used in search algorithms to represent the initial state, goal state, and current state of the problem. 
•	Each state in the state space is represented using a set of variables.{States.possible Action,Result,Cost}
•	The efficiency of the search algorithm greatly depends on the size of the state space,

1.2	Features 
•	Exhaustiveness:State space search explores all possible states of a problem to find a solution.
•	Precise: It give precise information about all the states so that machine is able to analyze it properly 
•	Completeness:If a solution exists, state space search will find it.
•	Optimality:Searching through a state space results in an optimal solution.
1.3	Example: 8 puzzle problem
The 8-Puzzle is a sliding puzzle that consists of a 3x3 grid with eight numbered tiles and one blank space. The goal is to rearrange the tiles from an arbitrary initial state to a goal state by sliding tiles into the blank space.
1.	State Representation:
The state is represented by the arrangement of the tiles on the board. For example:
Initial State:      Goal State:
1 2 3               1 2 3
4 5 6               8    4
7 8                  7 6 5 
2.	Operators/Actions:
Possible actions include moving a tile adjacent to the blank space into the blank space.example Upward,downward right left	
3.	Goal State:
The goal state is a predefined stae and check each state that it ias a goal state or not
4.	Cost: the amount of time/space/cost required to move from one state to another

1.4	Types of Space state search
Uninformed: Also called as blind search don’t have additional information about the problem domain. It just have goal and initial state 
Informed Search: uses additional information, such as heuristics, to guide the search process.
Breadth First Search 
•	It comes under uninformed search technique in which We don’t have the domain specific knowledge about the domain problem like what is the cost in reaching the goal state
•	Optimal: Here optimal means shortest value given to reach the goal state if value is constant or not it always provide optimal or shortest path to reach at goal state 
•	Complete: it always provides a solution to the propblem 
•	Shallow node:unlike dfs it travels through level by level which is very important as reaches to every shallowest (neighbour) node so if there is a node at a level it first not traverse deeper 
•	FIFO : Queue 
•	Example:


Disadvantages:
•	It requires lots of memory since each level of the tree must be saved into memory to expand the next level.
•	BFS needs lots of time if the solution is far away from the root node.
•	Complexity : O(bd). b branches D is depth
Depth First Search
• Complexity : O(bd). b branches D is depth
Best First Search 
 

•	Informed Search technique
•	Provides good solution not optimal solution 
•	Provides normal complexity but in worst case O(bd)
•	What happens in best first search is we start from A to the goal g and we will check for each value the cost of reaching up to the goal then we will check from the starting node A there are three nodes B C and D to move now we will check from B to G what is the cost from C to G what is the cost and from D to G what is the cost then we will take the least value of reaching to the node goal then now we have seen C has the least value that is 25 then we will not approach to B and D and we will not check on B and D this is called as informed or heuristic method so we will go through C and now from we will check from C there are three nodes E F and A but we will cancel that A because it is already been taken so we will put them in a closed then we will check from E to F and now we will take the least value from E to F so F has the least value then we will check from F to G and then this is on now we have reached from F2 as goal so this is how we have get our best first search
Hill climbing search 
•	Local search algorithm which continuously moves in the direction of increasing elevation/value to find the peak of the mountain or best solution to the problem. It terminates when it reaches a peak value where no neighbor has a higher value.
•	No backtracking 
•	doesn’t have knowledge of global domain they have knowledge of local  domain  also called greedy local search as it only looks to its good immediate neighbor state and not beyond that.
•	Hill Climbing is mostly used when a good heuristic is available.
•	we don't need to maintain and handle the search tree or graph as it only keeps a single current state.
•	Examples of Hill climbing algorithm is Traveling-salesman Problem in which we need to minimize the distance traveled by the salesman.
•	Problems
•	Local Maximum: A local maximum is a peak state in the landscape which is better than each of its neighboring states, but there is another state also present which is higher than the local maximum.
 Solution: Backtracking ,the algorithm can backtrack the search space and explore other paths by creating alist of available paths as well.
•	Plateau is the flat area of the search space in which all the neighbor states of the current state contains the same value, because of this algorithm does not find any best direction to move. A hill-climbing search might be lost in the plateau area.
Solution: Randomly select a state which is far away from the current state so it is possible that the algorithm could find non-plateau region.




•	Ridge is a special form of the local maximum. an area which is higher than its surrounding areas and here direction can’t be changed so if it is moving in A direction and gets the maximum and there exist B also but higher in different direction it will not be able to find the value 
Solution : Moving in diferent direction is the only solution


A* search Technique
•	Informed Search Technique
•	A* search is the most commonly known form of best-first search. 
•	It uses heuristic function h(n), and cost to reach the node n from the start state g(n).
•	A* search algorithm finds the shortest path through the search space using the heuristic function.
•	This search algorithm expands less search tree and provides optimal result faster.
•	In A* search algorithm, we use search heuristic as well as the cost to reach the node. Hence we can combine both costs as following, and this sum is called as a fitness number.
•	A* search algorithm is optimal and complete.

 
Initialization: {(S, 5)}
Iteration1: {(S--> A, 4), (S-->G, 10)}
Iteration2: {(SA--> C, 4), (SA-->B, 7)
Iteration3: {(SAC--->G, 6), (SAC--->D, 11), 
Iteration 4 will give the final result, as S--->A--->C--->G it provides the optimal path with cost 6.
AO* Algorithm
•	Gatesmasher ao* algo
•	Performs Best-first search. 
•	divides any given difficult problem into a smaller group of problems that are then resolved using the AND-OR graph concept. 
•	AND OR graphs are specialized graphs that are used in problems that can be divided into smaller problems. 
•	The AND side of the graph represents a set of tasks that must be completed to achieve the main goal, 
•	hile the OR side of the graph represents different methods for accomplishing the same main goal.

Example



Means End Analysis
•	mixture of the two directions is appropriate for solving a complex and large problem. Such a mixed strategy, make it possible that first to solve the major part of a problem and then go back and solve the small problems arise during combining the big parts of the problem. Such a technique is called Means-Ends Analysis.
•	Means-Ends Analysis is problem-solving techniques used in Artificial intelligence for limiting search in AI programs.
•	It is a mixture of Backward and forward search technique.
•	The MEA analysis process centered on the evaluation of the difference between the current state and goal state.
How means-ends analysis Works:The means-ends analysis process can be applied recursively for a problem
1.	Evaluate the difference between Initial State and final State.
2.	Select the various operators which can be applied for each difference.
3.	Apply the operator at each difference, which reduces the difference between the current state and goal state.

Constraint Satisfaction Problem 
Basically what happens
1.	There are three variables or components in CSP  =>V,D,C (define in below image)
2.	C is a constraint that are applied on V
3.	So example is also given in an image in which 
4.	Constraint V1 and V2 should not be qual 
5.	Domain can be for v1->1,2 || V2 ->2,4
6.	Variable is v1 and v2
7.	So problem is we have to satisfy  states or problem in given range and constraint

Unit-2
Forward Reasoning:
•	It involves starting with available information (facts or premises) and moving forward to draw conclusions or make predictions. 
•	Goal -oriented approach where the system uses existing knowledge to derive new knowledge or reach a specific goal.
•	Knowledge Base:
o	Forward reasoning relies on a knowledge base that contains a set of facts, rules, and relationships about the problem 
o	The knowledge base serves as the foundation for drawing conclusions.
•	Initial Facts:
o	The process begins with an initial set of known facts.
o	These facts can be directly observed or obtained from external sources.
•	Inference 
o	The inference engine is responsible for applying the rules and logical operations defined in the knowledge base to draw new conclusions.
•	Goal or Objective:
o	Forward reasoning often has a specific goal or objective. The system aims to derive new information that contributes to achieving the desired outcome.
Advantages 
•	Transparency: The reasoning process is transparent and easy to understand.
•	Efficiency: It can be computationally efficient, especially when the goal is well-defined.
•	Goal-Oriented: Well-suited where the objective is to reach a specific goal.
Limitations 
•	May Miss Alternatives: It may not explore alternative paths or consider other possibilities not covered by existing rules.
•	Dependency on Knowledge Base: The effectiveness depends on the completeness and accuracy of the knowledge base.
Backward Reasoning
•	System starts with a goal and works backward to determine what facts or conditions are needed to achieve that goal. 
•	Problem-solving approach that begins with the end objective and systematically reasons backward to identify the steps or conditions necessary to reach that objective.
•	Goal or Objective:
o	Backward reasoning begins with a specific goal or desired outcome that the system aims to achieve.
•	Knowledge Base:
o	The knowledge base contains a set of rules, facts, and relationships that describe the domain or problem space.
o	It serves as the foundation for determining the conditions required to achieve the goal.
•	Inference Engine:
o	The inference engine is responsible for applying rules and logical operations in reverse order, starting from the goal and working backward to the initial conditions.
•	Working Memory:
o	The system maintains a working memory that stores the current state of knowledge and the conditions being considered.
Advantages 
•	Goal-Oriented: Well-suited for tasks where the objective is to achieve a specific goal.
•	Efficient for Goal Achievement: Can be efficient when the goal is clearly defined, and the system needs to work backward to find the required conditions.
Limitations 
•	May Miss Alternative Paths: It may not explore alternative paths or consider other possibilities not covered by existing rules.
•	Dependency on Knowledge Base: The effectiveness depends on the completeness and accuracy of the knowledge base.

Monotonic Reasoning:
•	Form of reasoning where the addition of new information never causes previously drawn conclusions to be revised or invalidated. 
•	It follows a "more information is always better" principle.
•	Certainty and Stability: Conclusions reached through monotonic reasoning are considered stable and certain.
•	Inference Process: The inference process is straightforward and based on the available information at any given time.
•	Example:
o	"All humans are mortal" and "John is a human," then it can be concluded that "John is mortal." If later it is discovered that John is also a professor, this new information doesn't affect the previous conclusion.
Non-monotonic Reasoning:
•	Form of reasoning where the addition of new information may lead to the revision or abandonment of previously made conclusions. 
•	It allows for reasoning under uncertainty and the possibility of changing beliefs.
•	Reevaluation of Conclusions: New information may cause a reevaluation of previously drawn conclusions. Conclusions are not necessarily fixed.
•	Handling Uncertainty: Non-monotonic reasoning is particularly useful in handling situations where information is incomplete or uncertain.
•	Default Assumptions: It often involves default assumptions or rules that can be overridden by new information.
•	Example:
o	 "Birds can fly." Based on this, one might initially conclude that "Robins can fly." However, if later information is provided that "This specific robin is a mechanical toy," the previous conclusion may be revised because the default assumption about birds flying does not apply to mechanical toys.
Reasoning with uncertainties
Form of reasoning that deals with incomplete, imprecise, or uncertain information. 
•	Uncertainty Types:
o	Arises due to incomplete knowledge or information.
o	Arises due to inherent randomness or variability in a system.
•	Fuzzy Logic:
o	Fuzzy logic allows for the representation of degrees of truth or membership in a set.
o	It is used when information is imprecise and boundaries between categories are not well-defined.


1.1  What is Machine Learning
	Machine learning (ML) is a subdomain of artificial intelligence (AI) that focuses on developing systems that learn—or improve performance—based on the data they ingest. 
	Machine learning is a field of technology  that allows machine the ability  to learn and improve their performance on a specific task without being explicitly programmed
	Machine learning is like teaching a computer to perform a task by showing it a large number of examples. Instead of explicitly telling the computer how to do the task, we let machines to figure out the patterns and rules on its own by the given data.

1.2 How does Machine Learning work
A machine learning system builds prediction models, learns from previous data, and predicts the output of new data whenever it receives it. The amount of data helps to build a better model that accurately predicts the output, which in turn affects the accuracy of the predicted output.

1.3 Feature of Machine learning
	Machine can learn itself from past data and automatically improve.
	From the given dataset it detects various patterns on data.
	It will become more easy to target relatable customer base.
1.4 Why Machine learning 
	Data Abundance: In the digital age, vast amounts of data are generated every day. Machine learning algorithms can extract patterns from this data, which would be impossible for humans to process manually.
        By which many complex problems can be resolved
	Automation: Machine learning enables automation of tasks and processes. It can replace manual and repetitive tasks,
	Hidden Patterns: Businesses and services increasingly aim to provide personalized experiences to users by Finding hidden patterns and extracting useful information from the user data.
	Decision making in various sector including finance

1.5 Types of machine learning problems
Supervised Learning: 
	sample labelled data are provided to the machine learning system for training, and the system then predicts the output based on the training data.
	The system uses labelled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output.
	Types of problems: Classification (categorizing data into classes) and Regression (predicting a continuous value).
	Example: Example: Handwriting Recognition where 1,2,3,4,5 are labels and images are their corresponding output 

Unsupervised Learning: 
	Deals with unlabeled data. 
	The algorithm learns patterns and structures in the data without explicit guidance.
	input data does not have corresponding labels or categories
	algorithm needs to act on that data without any supervision
	Types of problems: Clustering (grouping similar data points) and Dimensionality Reduction (reducing the number of features while retaining important information). 
	Example: Customer Segmentation it group customers into segments like "high-value shoppers," "young and budget-conscious customers," and "occasional buyers. By finding patterns And this is used to target to specific customer groups based on their characteristics and behaviors. 
Reinforcement Learning: 
	feedback-based learning method,
	The agent learns automatically with these feedbacks and improves its performance.
	a learning agent gets a reward for each right action and gets a penalty for each wrong action. 
	The goal of an agent is to get the most reward points, and hence, it improves its performance.
	Types of problems: Games (e.g., chess, Go), robotics, and autonomous systems.
	The robotic dog, which automatically learns the movement of his arms,
1.6  Lifecycle of ml
	Gathering of data
        The goal of this step is to identify and obtain all data-related problems.
        We need to identify the different data sources, as data can be collected from various sources such as files, database, internet, or mobile devices
	Collect data
        The quantity and quality of the collected data will determine the efficiency of the output.
        The more will be the data, the more accurate will be the prediction.
        Integrate the data obtained from different sources
	Data preparation
        Put  data into a suitable place and prepare it to use in our machine learning training.
        In this step, first, we put all data together, and then randomize the ordering of data.
        This step can be further divided into two processes:
	Data exploration:
        It is used to understand the nature of data the characteristics, format, and quality of data.
        A better understanding of data leads to an effective outcome. 
        find Correlations, general trends, and outliers.
	Data pre-processing:
        changing raw data into a configuration reasonable for model training
        data cleaning to eliminate irregularities or blunders, standardization to scale data inside a particular reach, highlight scaling
	Data Wrangling
        Data wrangling is the process of cleaning and converting raw data into a useable format. 
        It is the process of cleaning the data, selecting the variable to use, and transforming the data in a proper format to make it more suitable for analysis in the next step. 
        It is one of the most important steps of the complete process.
        Cleaning of data is required to address the quality issues.
        Sometimes few of the data may not be useful. So they can have various issues, including:
        Missing Values
        Duplicate data
        Invalid data
        Noise
        various filtering techniques is used to  clean the data.
        It is mandatory to detect and remove the above issues because it can negatively affect the quality of the outcome.
	Data Analysis
        This step involves:
        Selection of analytical techniques
        Building models
        Review the result
        The aim of this step is to build a machine learning model to analyse  the data using various analytical techniques and review the outcome. 
        It starts with the determination of the type of the problems, where we select the machine learning techniques such as Classification, Regression, Cluster analysis, Association, etc. then build the model using prepared data, and evaluate the model.
        So here we take the data and use machine learning algorithms to build the model.
	 Train Model
        We train our model to improve its performance for better outcome of the problem.
        We provide datasets to train the model using various machine learning algorithms. 
        Training a model is required so that it can understand the various patterns, rules, and, features.

	 Test Model
        In this step, we check for the accuracy of our model by providing a test dataset to it.
        Testing the model determines the percentage accuracy of the model as per the requirement of project or problem.
7. Deployment
	Here we deploy the model in the real-world system.
	If the above-prepared model is producing an accurate result as per our requirement with acceptable speed, then we deploy the model in the real system. 
	Before deploying check whether it is improving its performance using available data or not. 
	The deployment phase is similar to making the final report for a project.

1.7 Data pre-processing (handling missing values)
https://www.javatpoint.com/data-preprocessing-machine-learning

1.8 labels and features
	Let's first build the model by training it. During training, we provide the model with features and after prediction what we get is label . 
	For example, if we have a dataset containing information like age, weight, height, gender, and calories burned, these are the features. 
	What we want to predict, in this case, is how many calories were burned. So, the calories burned would be our output, and we call it the label. 
	If we were to predict the gender, then gender would be our label
	Features: all the columns like age salary input 
	Labels: output data 



1.9 Steps Involved in Supervised Learning:
	First Determine the type of training dataset 
	Collect/Gather the labelled training data. 
	Split the training dataset into training dataset, test dataset, and validation dataset.
Example : house price :
rows : different houses
columns: different features : area , bedroom ,sale price
x: columns such as  area ,bedroom or independent variable
y= columns such as sale price that we will predict a dependent variable 
x_train: 80% of the x to teach the model 
y_train: 80% of the y to teach for the corresponding value of x
x_test : 20%  of the x to test 
y_test: predict the sale price(y) corresponding to its x_test 
while building/teaching model  we provide model with x_train. y_train , x_test then at the end we test the model by making it predict the x_test and check if its producing correct y_test 
	Determine the input features of the training dataset, which should have enough knowledge so that the model can accurately predict the output.
	Determine the suitable algorithm for the model, such as support vector machine, decision tree, etc.
	Execute the algorithm on the training dataset. Sometimes we need validation sets as the control parameters, which are the subset of training datasets.
	Evaluate the accuracy of the model by providing the test set. If the model predicts the correct output, which means our model is accurate.



	Regression Analysis
	Regression analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. 
	It helps us to understand how the value of the dependent variable is changing corresponding to an independent variable when other independent variables are held fixed. 
	It predicts continuous/real values such as temperature, age, salary, price, etc.
	Regression is a supervised learning technique which helps in finding the correlation between variables and enables us to predict the continuous output variable based on the one or more predictor variables.
	Regression shows a line or curve that passes through all the datapoints on target-predictor graph in such a way that the vertical distance between the datapoints and the regression line is minimum



	Terminologies Related to the Regression Analysis:
	Dependent Variable: The main factor in Regression analysis which we want to predict or understand is called the dependent variable. It is also called target variable.
	Independent Variable: The factors which affect the dependent variables or which are used to predict the values of the dependent variables are called independent variable, also called as a predictor.
	Outliers: Outlier is an observation which contains either very low value or very high value in comparison to other observed values. An outlier may hamper the result, so it should be avoided.
	Multicollinearity: If the independent variables are highly correlated with each other than other variables, then such condition is called Multicollinearity. It should not be present in the dataset, because it creates problem while ranking the most affecting variable.
	Underfitting and Overfitting: If our algorithm works well with the training dataset but not well with test dataset, then such problem is called Overfitting. And if our algorithm does not perform well even with training dataset, then such problem is called underfitting.

	Linear Regression
	Statistical method that is used for predictive analysis. 
	It makes predictions for continuous/real or numeric variables such as sales, salary, age, product price
	Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables. 
	Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.
	The linear regression model provides a sloped straight line representing the relationship between the variables. 

	Positive Linear Relationship: dependent variable increases on the Y-axis and independent variable increases on X-axis
	Negative Linear Relationship: dependent variable decreases on the Y-axis and independent variable increases on the X-axis
	Mathematically 	
y=mx+b
	Where:
	y is the dependent variable (the variable you are trying to predict).
	x is the independent variable (the feature used for prediction).
	m is the slope of the line, which represents the rate of change of y with respect to x.
	b is the y-intercept, which is the value of y when x is zero.
	A linear line showing the relationship between the dependent and independent variables is called a regression line or best fit line

The best fit line:
	It represents the linear equation that best fits a given set of data points.
	It ensures the error between predicted values and actual values should be minimized. 
	The best fit line will have the least error.
	The different values for weights gives a different line of regression
	So best values are calculated for best fit by using cost function
Cost Function
	Mathematical function that measures the difference between predicted values and the actual values in dataset.
	The purpose of a cost function is to quantify how well or poorly a machine learning model is performing in terms of its ability to make predictions. 
	The model aims to minimize this cost function during the training process by adjusting its internal parameters (weights and biases) to make better predictions.

Calculate the Cost Function (Mean Squared Error)
	The cost function (mean squared error) measures the difference between the predicted values and the actual values. 
	It is used to assess how well the best-fit line fits the data.
	The formula for mean squared error (MSE) is: 
Least square
	The Least Squares Method is used to derive a generalized linear equation between two variables, one of which is independent and the other dependent on the former
	A statistical method that is used to find the equation of the line of best fit related to the given data. This method aims at reducing the sum of squares of deviations as much as possible
	Limitation: The least squares method assumes that the data is evenly distributed and doesn’t contain any outliers for deriving a line of best fit
	Steps:
	Step 1: Denote the independent variable values as xi and the dependent ones as yi.
	Step 2: Calculate the average values of xi and yi as X and Y.
	Step 3: Presume the equation of the line of best fit as y = mx + c, where m is the slope of the line and c represents the intercept of the line on the Y-axis.
	Step 4: The slope m can be calculated from the following formula:
	m = [Σ (X – xi)×(Y – yi)]/ Σ(X – xi)2
	c = Y – mX
	Example :https://www.geeksforgeeks.org/least-square-method/

Residual Sum of Squares (RSS)
	Residual sum of squares is used to calculate the variance of the data in terms of error or residuals.
	It is used to calculate the error left between regression data and regression function after running the model. 
	The smaller the value of the residual sum of squares, the better the model.
	 	
	yi is the ith value of variable to be predicted,
	f(xi) is the predicted value, and
	n is the number of terms or variables9.

Regression Sum of Squares (SSR)
	The regression sum of squares measures how well the model is and how close is the predicted value to the expected value.
	 
	Xi is the ith observation of the set,
	 is the mean of the dataset, and
	n is the number of observations.

Total Sum of Squares (TSS)
	It is used to denote the amount of variation in the dependent variable. 
	It is the sum of the regression sum of squares and the residual sum of squares.
	It is calculated as:
	TSS = RSS + SSR

	In linear Regression the checks which ensures to get the best possible result from the given dataset. 
	Linear relationship between the features and target:
	Linear relationship between the dependent and independent variables.
	Small or no multicollinearity between the features:
	Multicollinearity means high-correlation between the independent variables. 
	Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables
	it is difficult to determine which predictor variable is affecting the target variable and which is not. 
Types of Linear Regression
	Simple Linear Regression:
	    If a single independent variable is used to predict the value of a numerical dependent variable
	Multiple Linear regression:
	    If more than one independent variable is used to predict the value of a numerical dependent variable

Simple Linear Regression
	Relationship between a dependent variable and a single independent variable. 
	The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression.
	Dependent variable must be a continuous/real value.
	The independent variable can be measured on continuous or categorical values.
	Objective
	Model the relationship between the two variables. Such as the relationship between Income and expenditure, experience and Salary, etc.
	Forecasting new observations. Such as Weather forecasting according to temperature, 
	can be represented using the below equation:
	y= a1x+c

Multiple Linear Regression
	Algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.
	Uses Two or more independent variables to predict the same dependent variable 
	The goal is to model a linear relationship between the dependent variable and multiple independent variables.
	The equation for multiple linear regression can be written as:
	y=b0+b1x1+b2x2+b3x3+…+bnxn

3.1 Classification 
	Supervised Learning technique that is used to identify the category of new observations on the basis of training data.
	Program learns from the given dataset and then classifies new observation into a number of classes or groups. Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc.
	Classes can be called as targets/labels or categories
	The output variable  is a category, not a value, such as "Green or Blue", "fruit or animal", etc. 
	It takes labelled input data, which means it contains input with the corresponding output.
	Discrete output function(y) is mapped to input variable(x).
	y=f(x), where y = categorical output  
	The main goal is to identify the category of a given dataset


	2 Learners in Classification Problems:
Lazy Learners: 
•	It firstly stores the training dataset and wait until it receives the test dataset.     
•	classification is done on the basis of the most related data stored in the training dataset. 
•	It takes less time in training but more time for predictions. Ex: K-NN algorithm
Eager Learners :
•	Eager Learners develop a classification model based on a training dataset before receiving a test dataset. 
•	Opposite to Lazy learners, Eager Learner takes more time in learning, and less time in prediction. 
•	Example: Decision Trees, Naïve Bayes, ANN.

3.3 Logistic Regression
•	Logistic regression predicts the output of a categorical dependent variable.
•	It doesn’t give exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.
•	instead of fitting a regression line, we fit an "S" shaped logistic function, which predicts two maximum values (0 or 1).
•	It has the ability to provide probabilities and classify new data using continuous and discrete datasets.






3.4 Sigmoid Function
•	The sigmoid function is a mathematical function used to map the predicted values to probabilities.
•	It maps any real value into another value within a range of 0 and 1.
•	The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the "S" form. The S-form curve is called the Sigmoid function or the logistic function.
•	In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0
•	equation of the straight line
 

•	y can be between 0 and 1 only, so for this let's divide the above equation by (1-y):
 
•	But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will become:

 




4.1 Confusion Matrix
•	The confusion matrix is a matrix used to determine the performance of the classification models for a given set of test data. 
•	It can only be determined if the true values for test data are known.
•	For  2 prediction class the matrix is of 2*2 table, for 3 classes, it is 3*3 table
•	The matrix is divided into two dimensions, that are predicted values and actual values along with the total number of predictions.
•	Predicted values are  values, predicted by the model, and actual values are the true values for the given observations.


•	Need for Confusion Matrix 
	It evaluates the performance of the classification models, when they make predictions on test data, and tells how good our classification model is.
	It not only tells the error made by the classifiers but also the type of errors such as it is either type-I or type-II error.
	With the help of the confusion matrix, we can calculate the different parameters for the model, such as accuracy, precision
•	True Negative: Model has given prediction No, and the real or actual value was also No.
•	True Positive: The model has predicted yes, and the actual value was also true.
•	False Negative: The model has predicted no, but the actual value was Yes, also called a Type-II error.
•	False Positive: The model has predicted Yes, but the actual value was No. also called a Type-I error.
•	Accuracy: Accuracy is a measure of how many correctly predicted instances there are out of the total instances. It's the most basic measure of a model's performance and is calculated as
o	Accuracy= Total Number of Predictions/Number of Correct Predictions

o	 
•	Precision measures the fraction of true positive predictions among all positive predictions. 
o	It's used when you want to minimize false positives.
o	Precision=True Positives/True Positives + False Positives
•	Recall (or Sensitivity) measures the fraction of true positive predictions among all actual positives. It's used when you want to minimize false negatives.
o	Recall = True Positives/True Positives +False Negatives
•	F1-Score: The F1-Score is the harmonic mean of precision and recall. 
o	It provides a balance between the two metrics and is useful when you want to find a good compromise between precision and recall.
o	F1-Score= 2⋅Precision⋅Recall/ Precision+Recall

	https://www.javatpoint.com/bias-and-variance-in-machine-learning

•	Bias:
•	Bias is a prediction error that is introduced in the model due to oversimplifying the machine learning algorithms. 
•	the difference between the predicted values and the actual values.
•	While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors 
•	Low Bias Models: Low bias models are flexible and make fewer assumptions about the data.
•	They have the capacity to capture complex patterns.
•	Overfitting Risk: When a model is very flexible (low bias), it can fit the training data extremely closely, even to the extent of capturing random fluctuations or noise in the data.
•	Capturing Noise: This means that the model doesn't just learn the meaningful patterns but also learns the irregularities and randomness present in the training data. 
•	It tries to explain every data point, even if some of them are just due to chance.

•	Variance: 
•	If the machine learning model performs well with the training dataset, but does not perform well with the test dataset, then variance occurs.
•	variance tells that how much a random variable is different from its expected value
•	Low variance means there is a small variation in the prediction of the target function with changes in the training data set.
•	 High variance shows a large variation in the prediction of the target function with changes in the training dataset.
•	A model that shows high variance learns a lot and perform well with the training dataset, and does not generalize well with the unseen dataset. As a result, such a model gives good results with the training dataset but shows high error rates on the test dataset.

	
                                      
	
